{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vessel Detection Dataset\n",
    "\n",
    "#### How to Create Query/Archive Sets\n",
    "\n",
    "This notebook creates a query/archive split for the vessel detection dataset. Images are grouped into subsets, depending on the number of vessels contained (here we consider {0}, {1,...5}, {6...}), and then split into one (two) thirds of the dataset such that splits become balanced with respect to #vessels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%matplotlib inline\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing 'vessels_dataset_annotations.json' as well as a folder\n",
    "# called 'tif' which holds all dataset images as *.tif\n",
    "root_dir = \"\"\n",
    "tif_dir = f\"{root_dir}/tif\"\n",
    "json_file = f\"{root_dir}/vessels_dataset_annotations.json\"\n",
    "\n",
    "with open(json_file) as f:\n",
    "    d = json.load(f)\n",
    "    annotations_dict = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to zero_patches.json (also included in this repo)\n",
    "zero_patches_json = f\"{...}/zero_patches.json\"\n",
    "\n",
    "with open(zero_patches_json) as json_data:\n",
    "    d = json.load(json_data)\n",
    "    zero_patches = d['zero_patches']\n",
    "    json_data.close()\n",
    "\n",
    "num_zero_patches = len(zero_patches)\n",
    "patches = [Path(file).stem for file in Path(tif_dir).glob(\"*.tif\") if Path(file).stem not in zero_patches] \n",
    "\n",
    "num_patches = len(patches)\n",
    "print(f\"{num_patches}/{num_patches+num_zero_patches}, e.g.'{patches[0]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_vessels(patch_name):\n",
    "    annotation = annotations_dict[patch_name]\n",
    "    return len(annotation['annotations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_num_vessels('T10SEG_20190808T184921_TCI_crop_x-1408_y-1152')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acquire number of vessels for all images in the dataset\n",
    "ll = np.zeros(shape=(num_patches, 2), dtype=object)\n",
    "\n",
    "for i, p in enumerate(patches):\n",
    "    ll[i][0] = p\n",
    "    ll[i][1] = get_num_vessels(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the dataset into 3 parts, according to number of vessels contained\n",
    "ll = np.asarray(ll)\n",
    "ll.shape\n",
    "\n",
    "s1 = np.where(ll[:, 1] == 0)[0]\n",
    "s2 = np.where((0 < ll[:, 1]) & (ll[:, 1] < 15))[0]\n",
    "s3 = np.where(15 <= ll[:, 1])[0]\n",
    "\n",
    "print(s1.shape, s2.shape, s3.shape, s1.shape[0] + s2.shape[0] + s3.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build query and archive sets such that both contain the same number of \n",
    "# vessels according our previous split into s1, s2, s3\n",
    "num_archive_s1 = int(s1.shape[0]*0.33)\n",
    "num_archive_s2 = int(s2.shape[0]*0.33)\n",
    "num_archive_s3 = int(s3.shape[0]*0.33)\n",
    "\n",
    "print(f\"Respective archive sizes (~two thirds): {num_archive_s1}/{s1.shape[0]}, {num_archive_s2}/{s2.shape[0]}, {num_archive_s3}/{s3.shape[0]}, {s1.shape[0] + s2.shape[0] + s3.shape[0]}\")\n",
    "\n",
    "# query-set should be 2/3 of all data\n",
    "s1_index = np.random.choice(s1.shape[0], num_archive_s1, replace=False)  \n",
    "s2_index = np.random.choice(s2.shape[0], num_archive_s2, replace=False)  \n",
    "s3_index = np.random.choice(s3.shape[0], num_archive_s3, replace=False)\n",
    "\n",
    "s1_a = s1[s1_index]\n",
    "s2_a = s2[s2_index]\n",
    "s3_a = s3[s3_index]\n",
    "print(\"Archive\", s1_a.shape, s2_a.shape, s3_a.shape, s1_a.shape[0] + s2_a.shape[0] + s3_a.shape[0])\n",
    "\n",
    "\n",
    "mask = np.ones(s1.size, dtype=bool)\n",
    "mask[s1_index] = False\n",
    "s1_q = s1[mask]\n",
    "\n",
    "mask = np.ones(s2.size, dtype=bool)\n",
    "mask[s2_index] = False\n",
    "s2_q = s2[mask]\n",
    "\n",
    "mask = np.ones(s3.size, dtype=bool)\n",
    "mask[s3_index] = False\n",
    "s3_q = s3[mask]\n",
    "print(\"Query\", s1_q.shape, s2_q.shape, s3_q.shape, s1_q.shape[0] + s2_q.shape[0] + s3_q.shape[0])\n",
    "\n",
    "\n",
    "query = np.concatenate((s1_q, s2_q, s3_q))\n",
    "archive = np.concatenate((s1_a, s2_a, s3_a))\n",
    "\n",
    "print()\n",
    "print('Final distribution query/archive', query.shape, archive.shape)\n",
    "print('Sanity check: First 10 query indices:', query[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index the corresponding query and archive patches to store the in two separate\n",
    "# json files\n",
    "patches = np.asarray(patches)\n",
    "query_patches = patches[query]\n",
    "archive_patches = patches[archive]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"Comment this line to write to disk.\"\n",
    "d_query = dict({ 'query_patches': query_patches.tolist() })\n",
    "d_archive = dict({ 'archive_patches': archive_patches.tolist() })\n",
    "\n",
    "with open(f'./query_patches.json', 'w', encoding='utf-8') as f:\n",
    "    print(len(list(d_query.keys())))\n",
    "    json.dump(d_query, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(f'./archive_patches.json', 'w', encoding='utf-8') as f:\n",
    "    print(len(list(d_query.keys())))\n",
    "    json.dump(d_archive, f, ensure_ascii=False, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
